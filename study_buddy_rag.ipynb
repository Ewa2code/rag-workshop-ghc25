{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  Build Your Own RAG Chatbot â€” Student Notebook (Simplified)\n",
        "\n",
        "You'll complete just the key parts of a RAG pipeline:\n",
        "\n",
        "âœ… retrieval (FAISS search)\n",
        "âœ… building the prompt\n",
        "âœ… calling the LLM\n",
        "âœ… testing your RAG chatbot\n",
        "\n",
        "Everything else is already handled for you.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai PyPDF2 faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from getpass import getpass\n",
        "import PyPDF2\n",
        "import faiss\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GEMINI_API_KEY = getpass(\"ðŸ”‘ Enter your Gemini API key: \")\n",
        "genai.configure(api_key=GEMINI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_name = list(uploaded.keys())[0]\n",
        "text = \"\"\n",
        "\n",
        "if file_name.endswith(\".pdf\"):\n",
        "    reader = PyPDF2.PdfReader(file_name)\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() or \"\"\n",
        "else:\n",
        "    text = uploaded[file_name].decode(\"utf-8\")\n",
        "\n",
        "print(f\"âœ… Loaded {len(text)} characters from {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, chunk_size=1000, overlap=200):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "chunks = split_text(text)\n",
        "print(f\"ðŸ“š Split into {len(chunks)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = \"models/gemini-embedding-001\"\n",
        "embeddings = []\n",
        "\n",
        "for chunk in chunks:\n",
        "    result = genai.embed_content(model=embed_model, content=chunk)\n",
        "    embeddings.append(result[\"embedding\"])\n",
        "\n",
        "embeddings = np.array(embeddings, dtype=\"float32\")\n",
        "\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "print(\"âœ… Vector index ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ… Step 7: Fill in the Retrieval + RAG Logic\n",
        "You're writing the MOST important part of a RAG system:\n",
        "\n",
        "1. Embed the user query\n",
        "2. Search the FAISS vector index\n",
        "3. Retrieve the top-k chunks\n",
        "4. Build the prompt\n",
        "5. Call the LLM\n",
        "\n",
        "Fill in the `YOUR CODE HERE` parts only.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, k=3):\n",
        "    # ðŸ‘‰ TODO: Embed the query\n",
        "    # q_embed = genai.embed_content(...)\n",
        "\n",
        "    # ðŸ‘‰ TODO: FAISS similarity search\n",
        "    # _, idx = index.search(...)\n",
        "\n",
        "    # ðŸ‘‰ TODO: return the matching chunks\n",
        "    # return [chunks[i] for i in idx[0]]\n",
        "    pass\n",
        "\n",
        "\n",
        "def ask_rag(query):\n",
        "    docs = retrieve(query)\n",
        "    context = \"\\n\\n\".join(docs)\n",
        "\n",
        "    # ðŸ‘‰ TODO: Write a good RAG prompt\n",
        "    prompt = f\"YOUR PROMPT HERE\"\n",
        "\n",
        "    # ðŸ‘‰ TODO: call the Gemini LLM\n",
        "    # model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "    # response = model.generate_content(prompt)\n",
        "\n",
        "    # ðŸ‘‰ TODO: return model output\n",
        "    # return response.text\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ… Step 8: Run your RAG chatbot!\n",
        "Put any question you want here.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"YOUR QUESTION HERE\"\n",
        "print(\"ðŸ¤” Q:\", question)\n",
        "\n",
        "# ðŸ‘‰ TODO: call ask_rag()\n",
        "# print(ask_rag(question))"
      ]
    }
  ],
  "metadata": {
    "colab": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
